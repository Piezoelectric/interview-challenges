{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef5fc76-9a3d-4264-9de9-efff5364247b",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Exploring a subset of the Enron emails. Done in a single day as part of a coding challenge/interview challenge.\n",
    "\n",
    "If I had more time, I'd like to \n",
    " * improve how I imported/tokenized the data, and reorganize the notebook so tokenizing happens upfront. I left it as-is to demonstrate my thought process and iterations as I worked.\n",
    " * explore the dataset outside of the NLP/Information Storage and Retrieval space / being a little less tunnel-visioned on that particular aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407f0af-6f47-4909-9111-625c60b2f6fa",
   "metadata": {},
   "source": [
    "# Imports and boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b44b43d-1b35-459f-810c-a3661232844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba80c30-1e71-48c9-bddb-aaf25735b660",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf03da99-21c6-4be3-9fba-637df1203043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      1000 non-null   object        \n",
      " 1   From      1000 non-null   object        \n",
      " 2   To        995 non-null    object        \n",
      " 3   Subject   698 non-null    object        \n",
      " 4   content   1000 non-null   object        \n",
      " 5   new_date  1000 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_orig = pd.read_csv('enron_test.csv', parse_dates=['Date', 'new_date'])\n",
    "# Date formats are recorded as 5/14/2001  23:39:00, which is MM/DD/YYYY. This should be the default arrangement\n",
    "df_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ea151d-169e-4248-a7b2-1b6d130baa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63384e8-b48b-467b-a601-2d8e6bccfdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_date'][0].month #more sancheck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5026c-16fc-48c1-87ab-ce605b4597f0",
   "metadata": {},
   "source": [
    "## Converting string \"frozenset\" into actual frozenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b01414-2acd-4640-a670-789e08f7c4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"frozenset({'phillip.allen@enron.com'})\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['From'][0] #== \"frozenset({'phillip.allen@enron.com'})\"\n",
    "# type(df['From'][0]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d4b6f2-c7fe-4365-b595-10870f9b38e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['From'] = df_orig['From'].apply(lambda x: eval(x), 'columns') #column axis, aka apply to each row\n",
    "# yes eval in production code is hideously dangerous, good thing this isn't production code\n",
    "type(df['From'][0]) #frozenset, as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3efa9b2-fc81-4da6-b787-c2a5721e0aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['To'][188]) #some To entries are null; for example this one is nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe5f841-03db-42f1-993b-51605ad78ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'frank.ermis@enron.com',\n",
       "           'jane.tholt@enron.com',\n",
       "           'jay.reitmeyer@enron.com',\n",
       "           'keith.holst@enron.com',\n",
       "           'matthew.lenhart@enron.com',\n",
       "           'mike.grigsby@enron.com',\n",
       "           'monique.sanchez@enron.com',\n",
       "           'randall.gay@enron.com',\n",
       "           'steven.south@enron.com',\n",
       "           'susan.scott@enron.com',\n",
       "           'tori.kuykendall@enron.com'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['To'] = df_orig['To'].apply(lambda x: eval(x) if type(x) == type(\"a\") else {}, 'columns') \n",
    "# dunno how to feel about this lambda. might rewrite to use pd.isnull\n",
    "df['To'][227]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8485cc4-7d76-4803-8a90-55fc78611d75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with forwarded emails\n",
    "\n",
    "Some emails were forwarded; for example, this one:\n",
    "\n",
    "```\n",
    "\"---------------------- Forwarded by Phillip K Allen/HOU/ECT on 10/16/2000 \n",
    "01:42 PM ---------------------------\n",
    "\n",
    "\n",
    "\"\"Buckner, Buck\"\" <buck.buckner@honeywell.com> on 10/12/2000 01:12:21 PM\n",
    "To: \"\"'Pallen@Enron.com'\"\" <Pallen@Enron.com>\n",
    "cc:  \n",
    "Subject: FW: fixed forward or other Collar floor gas price terms\n",
    "\n",
    "\n",
    "Phillip,\n",
    "\"\n",
    "```\n",
    "\n",
    "It may be worth creating some additional columns, if I have time later:\n",
    "* One to indicate if the email was forwarded (boolean true/false)\n",
    "* One to strip the \"forwarded\" boilerplate text (if necessary), leaving the actual message behind.\n",
    "  * Why strip the \"forwarded\" text? Because with the full text, the entity \"Phillip K Allen/HOU/ECT\" shows up a whole lot.\n",
    "  * Why keep the other text? One could argue that a forwarded message, being meant for a new audience, carries as much weight as the original message (even if the fwd'd message is literally the same as the original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981b0a43-dec7-4f4e-b0d1-4238c8f2d9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fw:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig['Subject'][9].lower()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988f53af-7027-40f6-b7bd-b6ea76ee407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Forwarded'] = df_orig['Subject'].apply(lambda x: True if (pd.notna(x) and x.lower()[:3] == 'fw:') else False)\n",
    "# Creates the \"Forwarded\" boolean column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fba0baa-16b3-4e45-b68d-886d4a79b545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Forwarded'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121661f6-af29-4987-9c1b-d56a7c1237d1",
   "metadata": {},
   "source": [
    "## Some additional data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf82d27-953d-4f9a-ac5f-43a63878a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content2'] = df_orig['content'].apply(lambda x: x.lower().replace(\"\\n\", \" \") if pd.notna(x) else None)\n",
    "#Lowercases everything. Replaces newlines with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95e9022-cd4d-4c6d-9852-b41a48861273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                here is our forecast   \n",
       "1      traveling to have a business meeting takes the...\n",
       "2                         test successful.  way to go!!!\n",
       "3      randy,   can you send me a schedule of the sal...\n",
       "4                    let's shoot for tuesday at 11:45.  \n",
       "                             ...                        \n",
       "995    jacques,  still trying to close the loop on th...\n",
       "996    larrry,  i realize you are disappointed about ...\n",
       "997    ---------------------- forwarded by phillip k ...\n",
       "998    jacques,  i think we reached an agreement with...\n",
       "999    ---------------------- forwarded by phillip k ...\n",
       "Name: content2, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c2b52-22c1-406a-94ad-e98ba3c1c60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Term Frequency\n",
    "\n",
    "Exploring how often certain terms or topics come up in the email bodies. Goal was to find the most discussed (and in some sense, most important) terms/topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed7c21f2-6db1-48b6-b329-d876036d33d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Apple': 1, 'U.K.': 1, '$1 billion': 1, 'San Francisco': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_0 = nlp(sentences[0])\n",
    "doc_2 = nlp(sentences[2])\n",
    "\n",
    "combined_entities = doc_0.ents + doc_2.ents \n",
    "texts = [x.text for x in combined_entities]\n",
    "Counter(texts)\n",
    "# Mostly I just wanted to make sure that this would work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8569c3-0e3d-40ca-83c0-c1e090b6b0f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Named entity recognition on the unedited emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdfb028b-3150-4d72-94ff-7cd1403b700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Entities'] = df['content'].apply(lambda x: nlp(x).ents)\n",
    "# warning - takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae3cafb-8b43-4da8-b6be-61d84f8564ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     ()\n",
       "1                                            ((Austin),)\n",
       "2                                                     ()\n",
       "3                                   ((Randy), (Phillip))\n",
       "4                                   ((Tuesday), (11:45))\n",
       "                             ...                        \n",
       "995    ((Jacques), (15,000), (today), (tomorrow), (Ph...\n",
       "996                                ((15,000), (Phillip))\n",
       "997    ((Phillip, K, Allen, /, HOU, /, ECT), (12/06/2...\n",
       "998    ((Jacques), (George), (Larry), (One), (15,000)...\n",
       "999    ((Phillip, K, Allen, /, HOU, /, ECT), (03/15/2...\n",
       "Name: Entities, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e15e1d0-211c-4e96-a391-bf97c0ae8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin PERSON\n"
     ]
    }
   ],
   "source": [
    "combined_entities = list(chain.from_iterable(df['Entities'])) \n",
    "# couldn't find a \"reduce\" function in pandas (as map-reduce), used this instead https://stackoverflow.com/a/35005105\n",
    "print(combined_entities[0].text, combined_entities[0].label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ca0796-ef75-4106-8091-54ba86ec516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Austin', 'PERSON')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_entities_pairs = [(x.text, x.label_) for x in combined_entities]\n",
    "# Convert things into pair form so they can be sorted by Counter() -- \n",
    "# The later approach doesn't need Counter, admittedly\n",
    "combined_entities_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2dbab67-acce-4b3a-acd4-070f18cd2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = [x.text for x in combined_entities]\n",
    "#Counter(texts).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0bae61c-09b5-4376-9d5e-c805f0cd8761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('09', 'CARDINAL'), 431),\n",
       " (('Phillip', 'PERSON'), 382),\n",
       " (('Phillip K Allen/HOU/ECT', 'PERSON'), 351),\n",
       " (('HOU/ECT@ECT', 'ORG'), 270),\n",
       " (('NA', 'ORG'), 204),\n",
       " (('20', 'CARDINAL'), 125),\n",
       " (('Phillip Allen', 'PERSON'), 116),\n",
       " (('Phillip K Allen/HOU/ECT@ECT\\n', 'PERSON'), 116),\n",
       " (('today', 'DATE'), 105),\n",
       " (('Enron', 'ORG'), 79),\n",
       " (('Phillip K Allen/HOU/ECT@ECT', 'PERSON'), 75),\n",
       " (('3', 'CARDINAL'), 71),\n",
       " (('1', 'CARDINAL'), 70),\n",
       " (('Lucy', 'PERSON'), 70),\n",
       " (('tomorrow', 'DATE'), 65),\n",
       " (('2', 'CARDINAL'), 63),\n",
       " (('Jeff', 'PERSON'), 62),\n",
       " (('one', 'CARDINAL'), 59),\n",
       " (('two', 'CARDINAL'), 58),\n",
       " (('ISO', 'ORG'), 54)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(combined_entities_pairs).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d3bd6-c74b-49d7-9e6b-a930b71cde4f",
   "metadata": {},
   "source": [
    "**What does this tell us?**\n",
    "\n",
    "It mostly tells us that Phillip K Allen was involved in a lot of these communications...which is not very surprising, considering so many of the emails were sent from, or directed to, Phillip. (In fact that's probably how this subset was sliced.)\n",
    "\n",
    "There's a lot of cardinals that can be stripped here, and specific token types (e.g. `ORG` for Organization) might be worth exploring further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb06f3e5-dac3-4d74-8128-15c598954035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_entities_pairs_no_cardinals = filter(lambda x: x[1] != 'CARDINAL', combined_entities_pairs)\n",
    "#Counter(combined_entities_pairs_no_cardinals).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "379c2f57-bea3-4c78-ba9d-9553d4d4f83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('HOU/ECT@ECT', 'ORG'), 270),\n",
       " (('NA', 'ORG'), 204),\n",
       " (('Enron', 'ORG'), 79),\n",
       " (('ISO', 'ORG'), 54),\n",
       " (('HOU/EES@EES', 'ORG'), 49),\n",
       " (('FERC', 'ORG'), 43),\n",
       " (('HOU/ECT', 'ORG'), 35),\n",
       " (('OWA', 'ORG'), 35),\n",
       " (('LLC', 'ORG'), 34),\n",
       " (('Enron North America Corp.', 'ORG'), 31),\n",
       " (('MW', 'ORG'), 31),\n",
       " (('HOU/ECT\\n', 'ORG'), 30),\n",
       " (('Yahoo', 'ORG'), 30),\n",
       " (('AES', 'ORG'), 28),\n",
       " (('HOU/ECT@ECT\\n', 'ORG'), 26),\n",
       " (('TDS', 'ORG'), 24),\n",
       " (('TX', 'ORG'), 21),\n",
       " (('p&l', 'ORG'), 21),\n",
       " (('SoCal', 'ORG'), 21),\n",
       " (('PDX/ECT@ECT', 'ORG'), 20)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_entities_pairs_orgs = filter(lambda x: x[1] == 'ORG', combined_entities_pairs)\n",
    "Counter(combined_entities_pairs_orgs).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7df2-b77b-4562-a0b9-734db86bdf27",
   "metadata": {},
   "source": [
    "**What does this tell us?**\n",
    "\n",
    "Some of the organizations named and involved in these emails. Some (e.g. `HOU/ECT@ETC`) are internal. Others (e.g. `yahoo`, `socal`) are not. Some (`AES`) are stock ticker symbols for other companies (which makes sense given Enron's financial dealings). \n",
    "\n",
    "This also calls to attention\n",
    "* the fact that I need to strip newlines from some of these, as `HOU/ECT@ECT` and `HOU/ECT@ETC\\n` were considered separate entities. And do a lot more data cleanup (e.g. forced lowercasing) \n",
    "  * done\n",
    "* the fact that AES is mentioned 28 times ... but only in 2 emails (one of which was a forward); therefore, putting AES as one of the \"most frequent\" topics mentioned isn't really representative of the underlying data.\n",
    "  * This particular insight (term frequency over the bag-of-words, vs *document frequency* (a 1/0 if the document contains/doesn't contain the term) motivated me to study IDF scores. \n",
    "  * However, IDF is *inverse* document frequency, prioritizing rarewords because they're interpreted as \"more informative\" (like \"arachnophobia\" being present in a webpage says more than \"the\" being present). \n",
    "  * Normal document frequency might be desired (and was calculated in a middle step)... in some sense, this was unnecessary for me to do by hand, as I could have leveraged spaCy to handle it automatically (by counting all things, not just Named Entities) -- however, it was nice to have in a vectorized/numpy-compatible form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee0bec-c9f8-461b-b77f-632f1802db73",
   "metadata": {},
   "source": [
    "## Named entity recognition on (slightly) cleaned up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "167d4e3d-3ece-48f9-91f5-9aba425a5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities2'] = df['content2'].apply(lambda x: nlp(x).ents)\n",
    "# Again, will take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2372eff0-b3b0-4256-bcad-b339e19c4640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('phillip k allen/hou/', 'PERSON'), 284),\n",
       " (('phillip k allen/hou/ect@ect', 'PERSON'), 195),\n",
       " (('today', 'DATE'), 125),\n",
       " (('enron', 'ORG'), 108),\n",
       " (('california', 'GPE'), 84),\n",
       " (('phillip k allen/hou/ect', 'PERSON'), 79),\n",
       " (('daily', 'DATE'), 76),\n",
       " (('hou/ect@ect', 'ORG'), 67),\n",
       " (('first', 'ORDINAL'), 63),\n",
       " (('tomorrow', 'DATE'), 61),\n",
       " (('friday', 'DATE'), 57),\n",
       " (('larry', 'PERSON'), 56),\n",
       " (('hr/corp/enron@enron', 'ORG'), 51),\n",
       " (('wednesday', 'DATE'), 50),\n",
       " (('monday', 'DATE'), 48),\n",
       " (('monthly', 'DATE'), 47),\n",
       " (('ferc', 'ORG'), 47),\n",
       " (('jeff', 'PERSON'), 47),\n",
       " (('thursday', 'DATE'), 44),\n",
       " (('tim belden/hou/ect@ect', 'PERSON'), 40)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_entities_2 = list(chain.from_iterable(df['entities2'])) \n",
    "combined_entities_pairs_2 = [(x.text, x.label_) for x in combined_entities_2]\n",
    "Counter(filter(lambda x: x[1] != 'CARDINAL', combined_entities_pairs_2)).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f96f3-bd4a-4809-b628-0c43a0a856a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TF/IDF \n",
    "\n",
    "Or some other algorithm that deals with *non*-named entities and concepts, e.g. \"trades\" and \"company\" and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737e491-e930-41f9-932e-fa9e5d345846",
   "metadata": {},
   "source": [
    "## A TF/IDF matrix\n",
    "\n",
    "More useful for information storage/retrieval contexts than for strictly analytical contexts. However, I thought it was neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97bfd9a8-86bf-452c-bb9d-03da55c07a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86bf05fb-039d-45b6-8c23-1df7b3eb9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_term(x):\n",
    "    is_stop = x.is_stop\n",
    "    is_blank = (len(x.lemma_.strip()) == 0)\n",
    "    is_single_char = (len(x.lemma_) == 1)\n",
    "    is_punct = (x.is_punct or x.text in ['<', '>'])\n",
    "    is_fwd = (x.text[:5] == \"-----\") #dunno if actually needed or if lemma takes care of it. w/e tho\n",
    "    #is_number = x.lemma_.isnumeric() #might want to keep numbers in emails about finance data...\n",
    "    return not (is_stop or is_blank or is_single_char or is_punct or is_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2ba2139-91ee-48ce-8f3c-b4b5daad0f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,-./:;<=>?[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punct_reference = string.punctuation.replace(\"'\", \"\").replace(\"@\", \"\") #want apostrophes for slang, @ for email addresses\n",
    "punct_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fce2ff6-99b4-46d8-b8b5-68b645aa891d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heya'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_punct(x):\n",
    "    translation_table = str.maketrans('', '', punct_reference)\n",
    "    return x.translate(translation_table)\n",
    "\n",
    "strip_punct(\")heya\")\n",
    "# from https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58dda07b-209d-4ab8-b025-715e51076e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(document):\n",
    "    tokens = nlp(document)\n",
    "    tokens = [strip_punct(token.lemma_) for token in tokens if valid_term(token)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a81eaa1-1156-4e52-bc5a-af62a7a66916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# thanks to https://towardsdatascience.com/lovecraft-with-natural-language-processing-part-3-tf-idf-vectors-8c2d4df98621 for the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7778493c-2a7e-442c-b0dd-d4ebdce9d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d346652-077c-4343-92be-b3379c34b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tfidf_vectorizer.fit_transform(df['content2'])\n",
    "# Might be waiting here for a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53b0e12c-3ff3-4d33-99af-943f5ef9974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"'em\",\n",
       " '0',\n",
       " '00',\n",
       " '000',\n",
       " '000000000000935',\n",
       " '000000000001282',\n",
       " '000000000009659',\n",
       " '000000000021442',\n",
       " '000119']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58dedfb8-f4e9-4e63-ad9a-5e706c7d82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(\n",
    "    result.todense().tolist(), columns=tfidf_vectorizer.get_feature_names()\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec11e823-3307-4bce-8b1e-016ff7b6a63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>'em</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000000000000935</th>\n",
       "      <th>000000000001282</th>\n",
       "      <th>000000000009659</th>\n",
       "      <th>000000000021442</th>\n",
       "      <th>000119</th>\n",
       "      <th>...</th>\n",
       "      <th>ziegelaar</th>\n",
       "      <th>zimin</th>\n",
       "      <th>zisman</th>\n",
       "      <th>zivic</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zub</th>\n",
       "      <th>zufferli</th>\n",
       "      <th>zuniga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          'em    0   00  000  000000000000935  000000000001282  \\\n",
       "0    0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "1    0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "2    0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "3    0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "4    0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "..   ...  ...  ...  ...  ...              ...              ...   \n",
       "995  0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "996  0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "997  0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "998  0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "999  0.0  0.0  0.0  0.0  0.0              0.0              0.0   \n",
       "\n",
       "     000000000009659  000000000021442  000119  ...  ziegelaar  zimin  zisman  \\\n",
       "0                0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "1                0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "2                0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "3                0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "4                0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "..               ...              ...     ...  ...        ...    ...     ...   \n",
       "995              0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "996              0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "997              0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "998              0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "999              0.0              0.0     0.0  ...        0.0    0.0     0.0   \n",
       "\n",
       "     zivic  zone  zoning  zoom  zub  zufferli  zuniga  \n",
       "0      0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "1      0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "2      0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "3      0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "4      0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "..     ...   ...     ...   ...  ...       ...     ...  \n",
       "995    0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "996    0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "997    0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "998    0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "999    0.0   0.0     0.0   0.0  0.0       0.0     0.0  \n",
       "\n",
       "[1000 rows x 8770 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c76c38-510c-48f7-8722-ea4ea251529e",
   "metadata": {},
   "source": [
    "## Calculate IDF\n",
    "\n",
    "Broadly speaking, a measure of how \"informative\" a term is. More useful for the given prompt \"What are the most commonly mentioned topics in the email body?\"\n",
    "\n",
    "* Calculate *document frequency* $df_t$\n",
    "* Use numpy to handle the conversion from $df_t$ to $idf_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8f86a9b-303c-4baa-aa0c-02a657f754da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73d62926-e6b8-4ff9-892c-289f457ff0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(input = 'content', tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b894ac94-808b-46eb-91ef-33982f31010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = count_vectorizer.fit_transform(df['content2'])\n",
    "# takes a hot minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4dedb2f-56b4-4b1f-a765-e54474f121c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', \"'em\", '0', ..., 'zub', 'zufferli', 'zuniga'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afbeebdb-8cc3-4062-bed9-92028cc087a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x8770 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 53440 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2307258-8893-4576-9f95-ad6aead2c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[:,1] #1000 rows (documents), 1 word. testing some stuff out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f8286e6-790d-4db3-afb8-e4b32bb7fc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[:,6700].count_nonzero()\n",
    "\n",
    "#loginjhtml has an actual frequency of 2 and a calculated frequency of 2 as well.\n",
    "#well, it's actually 0, but it's \"login.jhtml\" in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a898e71b-8f33-4199-b5e7-3f46381c135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'potentially'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()[6700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36b0eeb4-9f0a-4019-a7d2-e52bfc58e703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8770"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = np.apply_along_axis(np.count_nonzero, 0, result_2.todense())\n",
    "len(list(counts)) #check it's the right axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b6ace88-287b-450f-98f4-ac0671cb7d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'em</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoning</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zub</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zufferli</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuniga</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8770 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Document Frequency\n",
       "                           7\n",
       "'em                        2\n",
       "0                          5\n",
       "00                         9\n",
       "000                        3\n",
       "...                      ...\n",
       "zoning                     6\n",
       "zoom                       1\n",
       "zub                        2\n",
       "zufferli                   4\n",
       "zuniga                     2\n",
       "\n",
       "[8770 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df = pd.DataFrame(\n",
    "    counts,\n",
    "    index = count_vectorizer.get_feature_names_out(),\n",
    "    columns = ['Document Frequency']\n",
    ")\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "886d97e2-fb7b-4027-a470-a1426fcf15cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>phillip</th>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward</th>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hou</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>katie</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevail</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertiser</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertise</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>httpinteractivewsjcompjportfoliodisplaycgi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8770 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Frequency\n",
       "phillip                                                    732\n",
       "allen                                                      406\n",
       "forward                                                    360\n",
       "hou                                                        342\n",
       "subject                                                    340\n",
       "...                                                        ...\n",
       "katie                                                        1\n",
       "prevail                                                      1\n",
       "advertiser                                                   1\n",
       "advertise                                                    1\n",
       "httpinteractivewsjcompjportfoliodisplaycgi                   1\n",
       "\n",
       "[8770 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.sort_values(by = ['Document Frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853a817-477a-4386-b414-b9e2953c38b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert document frequency matrix to IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd19a087-79a2-49e8-b36d-ba51ad6f2c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8770,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_shape = counts.shape\n",
    "temp_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94f1ecc8-844b-48ed-9792-40d468e43ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_scores = np.log10(np.divide(np.full(temp_shape,1000), counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b83a8f52-f3e1-47a4-89f5-31f349865607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>2.154902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'em</th>\n",
       "      <td>2.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.301030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>2.045757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>2.522879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoning</th>\n",
       "      <td>2.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zub</th>\n",
       "      <td>2.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zufferli</th>\n",
       "      <td>2.397940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuniga</th>\n",
       "      <td>2.698970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8770 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF Score\n",
       "           2.154902\n",
       "'em        2.698970\n",
       "0          2.301030\n",
       "00         2.045757\n",
       "000        2.522879\n",
       "...             ...\n",
       "zoning     2.221849\n",
       "zoom       3.000000\n",
       "zub        2.698970\n",
       "zufferli   2.397940\n",
       "zuniga     2.698970\n",
       "\n",
       "[8770 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_scores_df = pd.DataFrame(\n",
    "    idf_scores,\n",
    "    index = count_vectorizer.get_feature_names_out(),\n",
    "    columns = ['IDF Score']\n",
    ")\n",
    "idf_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "28d38168-f541-4894-8986-0d10d2de5154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDF Score    000000000009659\n",
       "dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_scores_df.idxmax()\n",
    "# gets the index of the cell with the highest IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e61af873-7ab7-4064-9fc4-87fffff2cff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>portion09of</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aimster</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ain</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ain@worldnetattnet</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>0.468521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hou</th>\n",
       "      <td>0.465974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward</th>\n",
       "      <td>0.443697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allen</th>\n",
       "      <td>0.391474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phillip</th>\n",
       "      <td>0.135489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8770 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    IDF Score\n",
       "portion09of          3.000000\n",
       "film                 3.000000\n",
       "aimster              3.000000\n",
       "ain                  3.000000\n",
       "ain@worldnetattnet   3.000000\n",
       "...                       ...\n",
       "subject              0.468521\n",
       "hou                  0.465974\n",
       "forward              0.443697\n",
       "allen                0.391474\n",
       "phillip              0.135489\n",
       "\n",
       "[8770 rows x 1 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_scores_df.sort_values(by = ['IDF Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1b381-c1cf-43d4-8d21-ff2c2baeab52",
   "metadata": {},
   "source": [
    "# Use Cases/further explorations\n",
    "\n",
    "The Enron dataset has had a lot of historical usages, some of which include\n",
    "* as a training dataset for spam email classifiers\n",
    "* as a training dataset for predictive text algorithms\n",
    "* identifying financial fraud during the actual Enron case, aggregating actual trades and transactions made\n",
    "* classifying some emails as relevant to financial fraud, and others as irrelevant\n",
    "* visualizing connections between the various employees of Enron, and which ones formed cliques/social circles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
